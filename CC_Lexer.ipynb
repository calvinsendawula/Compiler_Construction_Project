{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1EzLV7tlRoLu"
      },
      "outputs": [],
      "source": [
        "#importing Python's RegEx library\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "96HDe6oHDeYC"
      },
      "outputs": [],
      "source": [
        "# Function to split input sentence into tokens (words)\n",
        "def lexer(sentence):\n",
        "    # Using regular expression to split the sentence into tokens\n",
        "    # The regular expression pattern includes common punctuation and space as delimiters\n",
        "    return re.split(r\"[,|.|\\?|(|)| ]\", sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RAYQEUY1WFgx"
      },
      "outputs": [],
      "source": [
        "# Function to split input into tokens (characters)\n",
        "def enhanced_lexer(input_str):\n",
        "    # Define regular expressions for relevant tokens\n",
        "    letter_pattern = re.compile(r'[a-zA-Z]')           # Letters\n",
        "    digit_pattern = re.compile(r'\\d')                   # Digits\n",
        "    space_pattern = re.compile(r'\\s')                   # Space\n",
        "    punctuation_pattern = re.compile(r'[.,;:!?()-]')    # Common punctuation\n",
        "\n",
        "    # Tokenize the input string\n",
        "    tokens = []\n",
        "    for char in input_str:\n",
        "        if letter_pattern.match(char):\n",
        "            tokens.append(('LETTER', char.lower()))     # If the character is a letter, append ('LETTER', lowercase_char)\n",
        "        elif digit_pattern.match(char):\n",
        "            tokens.append(('DIGIT', char))              # If the character is a digit, append ('DIGIT', char)\n",
        "        elif space_pattern.match(char):\n",
        "            tokens.append(('SPACE', char))              # If the character is a space, append ('SPACE', char)\n",
        "        elif punctuation_pattern.match(char):\n",
        "            tokens.append(('PUNCTUATION', char))        # If the character is punctuation, append ('PUNCTUATION', char)\n",
        "        else:\n",
        "            tokens.append(('SPECIAL_CHARACTERS', char))  # If the character is none of the above, append ('SPECIAL_CHARACTERS', char)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tg0qT1Hthc7m"
      },
      "outputs": [],
      "source": [
        "# Function to render tokens\n",
        "def renderTokens(tokens):\n",
        "    global token_counter\n",
        "    token_counter = 0\n",
        "    token_list = []\n",
        "\n",
        "    # Formatting row + column heading\n",
        "    print(\"=\" * 50)\n",
        "    print(\"|\" \"\\t\", \"TOKEN NO\", \"\\t\", \"|\", \"\\t\", \" TOKEN\", \"\\t\" , \"|\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for token in tokens:\n",
        "        if token != \"\":\n",
        "            token_counter += 1\n",
        "            token_list.append(token)\n",
        "\n",
        "            # Formatting token rows + columns\n",
        "            print(\"|\", \"\\t\", token_counter, \"\\t\\t\", \"|\", \"\\t\", token)\n",
        "            print(\"-\"*50)\n",
        "    token_counter = 0\n",
        "    return token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0T6tdzVgf5U",
        "outputId": "d13d8405-a0d6-41a0-fe90-8e9a278905de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a string: mamam pap\n",
            "==================================================\n",
            "|\t TOKEN NO \t | \t  TOKEN \t |\n",
            "==================================================\n",
            "| \t 1 \t\t | \t mamam\n",
            "--------------------------------------------------\n",
            "| \t 2 \t\t | \t pap\n",
            "--------------------------------------------------\n",
            "\n",
            "Token List:['mamam', 'pap']\n",
            "Tokens: ['mamam', 'pap']\n",
            "==================================================\n",
            "|\t TOKEN NO \t | \t  TOKEN \t |\n",
            "==================================================\n",
            "| \t 1 \t\t | \t ('LETTER', 'm')\n",
            "--------------------------------------------------\n",
            "| \t 2 \t\t | \t ('LETTER', 'a')\n",
            "--------------------------------------------------\n",
            "| \t 3 \t\t | \t ('LETTER', 'm')\n",
            "--------------------------------------------------\n",
            "| \t 4 \t\t | \t ('LETTER', 'a')\n",
            "--------------------------------------------------\n",
            "| \t 5 \t\t | \t ('LETTER', 'm')\n",
            "--------------------------------------------------\n",
            "| \t 6 \t\t | \t ('SPACE', ' ')\n",
            "--------------------------------------------------\n",
            "| \t 7 \t\t | \t ('LETTER', 'p')\n",
            "--------------------------------------------------\n",
            "| \t 8 \t\t | \t ('LETTER', 'a')\n",
            "--------------------------------------------------\n",
            "| \t 9 \t\t | \t ('LETTER', 'p')\n",
            "--------------------------------------------------\n",
            "\n",
            "Token List:[('LETTER', 'm'), ('LETTER', 'a'), ('LETTER', 'm'), ('LETTER', 'a'), ('LETTER', 'm'), ('SPACE', ' '), ('LETTER', 'p'), ('LETTER', 'a'), ('LETTER', 'p')]\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "user_input = input(\"Enter a string: \")\n",
        "\n",
        "# Tokenize the user input using the normal lexer function\n",
        "tokens = lexer(user_input)\n",
        "token_list = renderTokens(tokens)\n",
        "print(f\"\\nToken List: {token_list}\")\n",
        "\n",
        "# Tokenize the user input using the enhanced_lexer function\n",
        "print(\"Tokens:\", tokens)\n",
        "en_tokens = enhanced_lexer(user_input)\n",
        "en_token_list = renderTokens(en_tokens)\n",
        "print(f\"\\nToken List: {en_token_list}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
